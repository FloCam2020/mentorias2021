{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc-autonumbering": true,
    "colab": {
      "name": "Mentoria_Practico_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DApFPJlAnZer"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report"
      ],
      "id": "DApFPJlAnZer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCUhYFlDMULE"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "tCUhYFlDMULE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zljLX_rNnZev"
      },
      "source": [
        "# Opción para ver todas las columnas del dataset en el notebook\n",
        "pd.set_option('display.max_columns', 50)"
      ],
      "id": "zljLX_rNnZev",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe3pMCpnZew"
      },
      "source": [
        "# Práctico 04: Aprendizaje Supervisado"
      ],
      "id": "hKe3pMCpnZew"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boZ8PSfTnZex"
      },
      "source": [
        "Para finalizar nuestro modelo, aplicaremos estrategias de sampling para dividir entre train y test y haremos crossvalidation sobre train. Realizaremos pruebas con varios clasificadores y evaluaremos los resultados con múltiples métricas. Por último calcularemos el feature importance y obtendremos conclusiones."
      ],
      "id": "boZ8PSfTnZex"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUu2EEnlnZex"
      },
      "source": [
        "## Objetivo del práctico"
      ],
      "id": "EUu2EEnlnZex"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isutnJ4VnZey"
      },
      "source": [
        "### Train-Validation-Test\n",
        "(obtener del práctico anterior)\n",
        "- División del dataset en train/validation/test\n",
        "- Estratificación"
      ],
      "id": "isutnJ4VnZey"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwaoEF6QnZey"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/AgusCarchano/Mentorias-grupo1/master/data/bank-additional-full.csv\"\n",
        "df = pd.read_csv(url, sep=\";\")"
      ],
      "id": "hwaoEF6QnZey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBmnvVJPnZez"
      },
      "source": [
        "# Reemplazamos la columna y (target) por 1 y 0\n",
        "df.y = df.y.replace('yes', 1)\n",
        "df.y = df.y.replace('no', 0)"
      ],
      "id": "VBmnvVJPnZez",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HjPIPM8nZez"
      },
      "source": [
        "#Diferenciamos los atributos del target\n",
        "X = df.drop(columns='y')\n",
        "y = df.y"
      ],
      "id": "3HjPIPM8nZez",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CarrvxpenZe0"
      },
      "source": [
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y)   #Dejamos un conjunto de test con el 20% de los casos \n",
        "#Dado que el dataset se encuentra desbalanceado (aprox. 11% del total de casos pertenece a la clase 1), empleamos el parámetro stratified en función del target (y)\n",
        "#De este modo, la muestra seleccionada a partir de la división sería representativa para las dos clases"
      ],
      "id": "CarrvxpenZe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8r3L1zcnZe0"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, stratify=y_temp)"
      ],
      "id": "i8r3L1zcnZe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abDV6qJ75yR_"
      },
      "source": [
        "#Pruebo sin hacer la doble division\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)"
      ],
      "id": "abDV6qJ75yR_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6_KjyfknZe1"
      },
      "source": [
        "### Preprocesamiento\n",
        "\n",
        "- Tratamiento de valores nulos\n",
        "- Estandarización\n",
        "- Encoding de variables categóricas"
      ],
      "id": "Q6_KjyfknZe1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2svohT6GnZe1"
      },
      "source": [
        "class SelectColumnsTransformer():\n",
        "    def __init__(self, columns=None):\n",
        "        self.columns = columns\n",
        "\n",
        "    def transform(self, X, **transform_params):\n",
        "        cpy_df = X[self.columns].copy()\n",
        "        return cpy_df\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self"
      ],
      "id": "2svohT6GnZe1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe59cnornZe2"
      },
      "source": [
        "#### Pipeline para el caso de clasificadores que requieren el escalado de variables (SVM, SGDClassifier)"
      ],
      "id": "Fe59cnornZe2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giAdXjHynZe2"
      },
      "source": [
        "#Aplicamos las transformaciones previas a los conjuntos de Train y Validation\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
        "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
        "mice_imputer = IterativeImputer(random_state=0, estimator=KNeighborsRegressor(),n_nearest_features=5)\n",
        "\n",
        "# Filtramos las variables que seleccionamos\n",
        "X_t = X_train[variables_categoricas + variables_numericas]\n",
        "X_v = X_val[variables_categoricas + variables_numericas]     \n",
        "\n",
        "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
        "                             ('standard_scaler', StandardScaler())      \n",
        "                            ])\n",
        "\n",
        "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
        "                                  ('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),\n",
        "                                 ('cat', OneHotEncoder())])\n",
        "\n",
        "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
        "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
        "                                      ])\n",
        "\n",
        "\n",
        "train = pipeline_completo.fit_transform(X_t)\n",
        "val = pipeline_completo.fit_transform(X_v)"
      ],
      "id": "giAdXjHynZe2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuTg5--UnZe3"
      },
      "source": [
        "#### Pipeline para el caso de clasificadores que no requieren el escalado de variables"
      ],
      "id": "AuTg5--UnZe3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI42n1TQnZe4"
      },
      "source": [
        "#Aplicamos las transformaciones previas a los conjuntos de Train y Test\n",
        "\n",
        "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
        "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
        "\n",
        "# Filtramos las variables que seleccionamos\n",
        "X_t = X_train[variables_categoricas + variables_numericas]\n",
        "X_v = X_val[variables_categoricas + variables_numericas]\n",
        "\n",
        "\n",
        "pipeline_numerico = Pipeline([\n",
        "                             ('select_numeric_columns', SelectColumnsTransformer(variables_numericas))     #Para este modelo no es necesario escalar las variables numéricas\n",
        "                            ])\n",
        "\n",
        "pipeline_categorico = Pipeline ([('imputer', SimpleImputer(strategy='most_frequent', missing_values = 'unknown')),\n",
        "                                   ('cat', OneHotEncoder())])\n",
        "\n",
        "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
        "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
        "                                      ])\n",
        "\n",
        "\n",
        "train = pipeline_completo.fit_transform(X_t)\n",
        "val = pipeline_completo.fit_transform(X_v)"
      ],
      "id": "bI42n1TQnZe4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxmdYFO2nZe8"
      },
      "source": [
        "### Definición de métricas"
      ],
      "id": "fxmdYFO2nZe8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GifGaktnZe8"
      },
      "source": [
        "Definiremos las métricas a utilizar:\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1\n",
        "- AUC\n",
        "- PRAUC  \n",
        "\n",
        "Además investigaremos como utilizar el classification report y confusion matrix. Adicionalmente, cómo usar crossvalidation."
      ],
      "id": "9GifGaktnZe8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo-BN4CGnZe8"
      },
      "source": [
        "### Testeo con varios modelos"
      ],
      "id": "vo-BN4CGnZe8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FP6YAl-nZe9"
      },
      "source": [
        "Realizaremos varios tests con diversos tipos de modelos de scikit-learn:\n",
        "- Logistic regression\n",
        "- SVM\n",
        "- Naive Bayes\n",
        "- etc  \n",
        "Usaremos crossvalidation y compararemos con validation y test."
      ],
      "id": "8FP6YAl-nZe9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUU3UtYtnZe9"
      },
      "source": [
        "### Modelos Tree Based"
      ],
      "id": "zUU3UtYtnZe9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewc_nZ18nZe-"
      },
      "source": [
        "En esta instancia utilizaremos modelos que no pertenecen a la librería scikit-learn.  \n",
        "Estos modelos son los más utilizados actualmente y han demostrado su efectividad en muchas competencias de Kaggle.  \n",
        "Además, tienen la ventaja de que \n",
        "- XGBoost\n",
        "- LightGBM"
      ],
      "id": "Ewc_nZ18nZe-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiYlWrwznZe-"
      },
      "source": [
        "### Optimización de Hiperparámetros"
      ],
      "id": "ZiYlWrwznZe-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ovVe2inZe-"
      },
      "source": [
        "En esta sección realizaremos varios tipos de optimización de hiperparámetros para lograr mejorar nuestras métricas.\n",
        "- Grid Search\n",
        "- Randomized Search"
      ],
      "id": "d8ovVe2inZe-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jG5rDkv5hCx"
      },
      "source": [
        "#### Random Forest"
      ],
      "id": "2jG5rDkv5hCx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vVgHv2v5efo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0bdf05-60d9-44f0-88fb-0b20b421f9d2"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "search_params = {\n",
        "    'n_estimators': [100, 500, 1000],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [3, 6, 10, None],\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf':[1,2,3]\n",
        "}\n",
        "\n",
        "forest_clf = RandomForestClassifier(random_state=42)\n",
        "forest_clf = GridSearchCV(forest_clf, search_params, cv=5, scoring='f1', n_jobs=-1)\n",
        "forest_clf.fit(train, y_train)"
      ],
      "id": "-vVgHv2v5efo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                              class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              max_samples=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              n_estimators=100, n_jobs=None,\n",
              "                                              oob_score=False, random_state=42,\n",
              "                                              verbose=0, warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'criterion': ['gini', 'entropy'],\n",
              "                         'max_depth': [3, 6, 10, None],\n",
              "                         'min_samples_leaf': [1, 2, 3],\n",
              "                         'min_samples_split': [2, 3, 4],\n",
              "                         'n_estimators': [100, 500, 1000]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='f1', verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDXG4JmBlfAf"
      },
      "source": [
        "display ('Best configuraton:')\n",
        "display(forest_clf.best_params_)\n",
        "\n",
        "best_forest_clf = forest_clf.best_estimator_"
      ],
      "id": "hDXG4JmBlfAf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJkPgE4tgp01"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_clf = RandomForestClassifier(random_state=42,\n",
        "                                    criterion = 'gini',\n",
        "                                    max_depth = 15,\n",
        "                                    min_samples_leaf = 1,\n",
        "                                    min_samples_split = 2,\n",
        "                                    n_estimators = 500)\n"
      ],
      "id": "HJkPgE4tgp01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3BGv74hgygv",
        "outputId": "ff81013e-920a-443e-e9ea-29e88ac29b9b"
      },
      "source": [
        "forest_clf.fit(train, y_train)"
      ],
      "id": "M3BGv74hgygv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=15, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
              "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfRVhzleg6Jl",
        "outputId": "78b332e0-69ac-44cf-c1c5-1af12c230904"
      },
      "source": [
        "#Predecimos y obtenemos las métricas de este modelo\n",
        "y_train_pred= forest_clf.predict(train)\n",
        "y_val_pred=forest_clf.predict(val)\n",
        "\n",
        "print(\"ENTRENAMIENTO\")\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "print(\"VALIDACIÓN\")\n",
        "print(classification_report(y_val, y_val_pred))"
      ],
      "id": "JfRVhzleg6Jl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENTRENAMIENTO\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97     29238\n",
            "           1       0.97      0.45      0.62      3712\n",
            "\n",
            "    accuracy                           0.94     32950\n",
            "   macro avg       0.95      0.73      0.79     32950\n",
            "weighted avg       0.94      0.94      0.93     32950\n",
            "\n",
            "VALIDACIÓN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.98      0.95      7310\n",
            "           1       0.64      0.24      0.35       928\n",
            "\n",
            "    accuracy                           0.90      8238\n",
            "   macro avg       0.78      0.61      0.65      8238\n",
            "weighted avg       0.88      0.90      0.88      8238\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT3TWoGAnZe_"
      },
      "source": [
        "#### SGDClassifier"
      ],
      "id": "XT3TWoGAnZe_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJk0758LnZe_",
        "outputId": "0de92414-6479-4f4f-95fe-c7e4cbfb41af"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "parameters = {'model__loss':['hinge', 'log','squared_loss'], 'model__learning_rate':['constant', 'optimal', 'invscaling', 'adaptive'], \n",
        "              'model__penalty': ['l2', 'l1', 'elasticnet'], 'model__eta0': [ 1e-6,0.001, 0.01, 0.1, 1, 10]}\n",
        "\n",
        "\n",
        "pipeline = Pipeline([('model',SGDClassifier( random_state= 1))])\n",
        "clf = GridSearchCV(pipeline, parameters, scoring=('f1','roc_auc'), cv = 5,return_train_score=True, refit='f1')\n",
        "\n",
        "clf.fit(train, y_train)"
      ],
      "id": "zJk0758LnZe_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('model',\n",
              "                                        SGDClassifier(alpha=0.0001,\n",
              "                                                      average=False,\n",
              "                                                      class_weight=None,\n",
              "                                                      early_stopping=False,\n",
              "                                                      epsilon=0.1, eta0=0.0,\n",
              "                                                      fit_intercept=True,\n",
              "                                                      l1_ratio=0.15,\n",
              "                                                      learning_rate='optimal',\n",
              "                                                      loss='hinge',\n",
              "                                                      max_iter=1000,\n",
              "                                                      n_iter_no_change=5,\n",
              "                                                      n_jobs=None, penalty='l2',\n",
              "                                                      power_t=0.5,\n",
              "                                                      random_state=1,\n",
              "                                                      shuffle=True,...\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'model__eta0': [1e-06, 0.001, 0.01, 0.1, 1, 10],\n",
              "                         'model__learning_rate': ['constant', 'optimal',\n",
              "                                                  'invscaling', 'adaptive'],\n",
              "                         'model__loss': ['hinge', 'log', 'squared_loss'],\n",
              "                         'model__penalty': ['l2', 'l1', 'elasticnet']},\n",
              "             pre_dispatch='2*n_jobs', refit='f1', return_train_score=True,\n",
              "             scoring=('f1', 'roc_auc'), verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "rXmHRdAwnZe_",
        "outputId": "eef2aaf9-83f2-410a-c82a-cf1906940d00"
      },
      "source": [
        "display ('Best configuraton:')\n",
        "display(clf.best_params_)\n",
        "\n",
        "best_sgd_clf = clf.best_estimator_\n"
      ],
      "id": "rXmHRdAwnZe_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Best configuraton:'"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'model__eta0': 10,\n",
              " 'model__learning_rate': 'constant',\n",
              " 'model__loss': 'hinge',\n",
              " 'model__penalty': 'l1'}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhfay_oqnZfA",
        "outputId": "7a3783ce-74a9-455d-9735-11feda618b1e"
      },
      "source": [
        "#Predecimos y obtenemos las métricas de este modelo\n",
        "y_train_pred= best_sgd_clf.predict(train)\n",
        "y_val_pred=best_sgd_clf.predict(val)\n",
        "\n",
        "print(\"ENTRENAMIENTO\")\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "print(\"VALIDACIÓN\")\n",
        "print(classification_report(y_val, y_val_pred))"
      ],
      "id": "nhfay_oqnZfA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ENTRENAMIENTO\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      1.00      0.94     23390\n",
            "           1       0.33      0.02      0.03      2970\n",
            "\n",
            "    accuracy                           0.89     26360\n",
            "   macro avg       0.61      0.51      0.49     26360\n",
            "weighted avg       0.83      0.89      0.84     26360\n",
            "\n",
            "VALIDACIÓN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      1.00      0.94      5848\n",
            "           1       0.31      0.02      0.03       742\n",
            "\n",
            "    accuracy                           0.89      6590\n",
            "   macro avg       0.60      0.51      0.48      6590\n",
            "weighted avg       0.82      0.89      0.84      6590\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BULyWdkpnY9C"
      },
      "source": [
        "#### Naive bayes"
      ],
      "id": "BULyWdkpnY9C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KGpQ8uQk8kH"
      },
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB"
      ],
      "id": "1KGpQ8uQk8kH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNxPS2QNkTDY"
      },
      "source": [
        "variables_categoricas = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
        "variables_numericas = ['age', 'campaign', 'previous', 'cons.conf.idx', 'euribor3m']\n",
        "\n",
        "# Filtramos las variables que seleccionamos\n",
        "X_t = X_train[variables_categoricas + variables_numericas]\n",
        "X_v = X_val[variables_categoricas + variables_numericas]\n",
        "\n",
        "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
        "                             # ('standard_scaler', StandardScaler()),\n",
        "                             (\"kbins_discretizer\", KBinsDiscretizer(n_bins=4, encode=\"ordinal\", strategy=\"uniform\")),   #strategy=\"uniform\"\n",
        "                             ('bins_cat', OneHotEncoder())\n",
        "                            ])\n",
        "\n",
        "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
        "                                 ('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),      #podríamos no ponerlo, y que deje \"desconocido\" como una categoría más\n",
        "                                 ('cat', OneHotEncoder())\n",
        "                                 ])\n",
        "\n",
        "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
        "                                   ('cat', pipeline_categorico, variables_categoricas),\n",
        "                                  ])\n",
        "\n",
        "pipeline_modelo = Pipeline([('preprocess', pipeline_completo),\n",
        "                            ('nb', ComplementNB())])\n",
        "\n",
        "#The Complement Naive Bayes classifier was designed to correct the “severe assumptions” made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.\n",
        "#En el pre-procesamiento transformé todos los atributos en categóricos, porque es el requerimiento del tipo de modelo"
      ],
      "id": "oNxPS2QNkTDY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-F8hB7FkYbw"
      },
      "source": [
        "#Solo Pre-procesamiento\n",
        "train = pipeline_completo.fit_transform(X_t)\n",
        "val = pipeline_completo.transform(X_v)"
      ],
      "id": "D-F8hB7FkYbw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8hZdetakdCX",
        "outputId": "8df41cee-7b2c-4f85-e612-0a892acf4811"
      },
      "source": [
        "#Modelo más sencillo\n",
        "nb=ComplementNB()\n",
        "nb.fit(train, y_train)"
      ],
      "id": "j8hZdetakdCX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogf6EaZ5kgvL",
        "outputId": "57f7af16-c82b-4b19-a463-e6387b5c282f"
      },
      "source": [
        "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
        "print(classification_report(y_train, nb.predict(train)))\n",
        "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
        "print(classification_report(y_val, nb.predict(val)))"
      ],
      "id": "ogf6EaZ5kgvL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MÉTRICAS CONJUNTO DE TRAIN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.74      0.83     23390\n",
            "           1       0.25      0.68      0.37      2970\n",
            "\n",
            "    accuracy                           0.73     26360\n",
            "   macro avg       0.60      0.71      0.60     26360\n",
            "weighted avg       0.87      0.73      0.78     26360\n",
            "\n",
            "MÉTRICAS CONJUNTO DE VALIDACIÓN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.75      0.84      5848\n",
            "           1       0.26      0.68      0.37       742\n",
            "\n",
            "    accuracy                           0.74      6590\n",
            "   macro avg       0.60      0.72      0.61      6590\n",
            "weighted avg       0.87      0.74      0.79      6590\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q677M0Kkmko"
      },
      "source": [
        "#Grilla de parámetros para optimización\n",
        "params={'alpha':[1, 0.5, 0.3, 0.1, 1.2, 1.5],\n",
        "        'fit_prior':[True, False],\n",
        "        'norm':[True,False]\n",
        "       }\n",
        "\n",
        "nb=ComplementNB()"
      ],
      "id": "8Q677M0Kkmko",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odRyJHVckrr7",
        "outputId": "1a06a03a-164d-4963-f2db-1141f23fa53a"
      },
      "source": [
        "#Búsqueda de los mejores parámetros\n",
        "cv_nb = GridSearchCV(nb, params, scoring='f1', cv=5,refit=True,n_jobs=-1)     \n",
        "cv_nb.fit(train, y_train)"
      ],
      "id": "odRyJHVckrr7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=ComplementNB(alpha=1.0, class_prior=None, fit_prior=True,\n",
              "                                    norm=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'alpha': [1, 0.5, 0.3, 0.1, 1.2, 1.5],\n",
              "                         'fit_prior': [True, False], 'norm': [True, False]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='f1', verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17185s_wkxE2",
        "outputId": "559064c5-a77c-4a78-d77e-ca6eed3795d1"
      },
      "source": [
        "cv_nb.best_params_"
      ],
      "id": "17185s_wkxE2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 0.3, 'fit_prior': True, 'norm': True}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXyMNRXCkz0s",
        "outputId": "e7c8c683-5bae-41bf-a8d5-7c18718ddf25"
      },
      "source": [
        "#Entrenamiento de la mejor versión encontrada del modelo\n",
        "nb_best = ComplementNB(alpha=0.3, fit_prior=True, norm=True)\n",
        "nb_best.fit(train, y_train)"
      ],
      "id": "DXyMNRXCkz0s",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ComplementNB(alpha=0.3, class_prior=None, fit_prior=True, norm=True)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeolnXIik148",
        "outputId": "dd8d3473-5b20-413b-ee6c-059ada1c943e"
      },
      "source": [
        "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
        "print(classification_report(y_train, nb_best.predict(train)))\n",
        "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
        "print(classification_report(y_val, nb_best.predict(val)))"
      ],
      "id": "aeolnXIik148",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MÉTRICAS CONJUNTO DE TRAIN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.92     23390\n",
            "           1       0.37      0.44      0.40      2970\n",
            "\n",
            "    accuracy                           0.85     26360\n",
            "   macro avg       0.65      0.67      0.66     26360\n",
            "weighted avg       0.86      0.85      0.86     26360\n",
            "\n",
            "MÉTRICAS CONJUNTO DE VALIDACIÓN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.92      5848\n",
            "           1       0.38      0.41      0.39       742\n",
            "\n",
            "    accuracy                           0.86      6590\n",
            "   macro avg       0.65      0.66      0.66      6590\n",
            "weighted avg       0.86      0.86      0.86      6590\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rldLCTDl32W"
      },
      "source": [
        "#### XGBoost"
      ],
      "id": "0rldLCTDl32W"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGElyCK0nu0O"
      },
      "source": [
        "import xgboost as xgb"
      ],
      "id": "lGElyCK0nu0O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJFPo3GZl3Pj"
      },
      "source": [
        "variables_categoricas = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
        "variables_numericas = ['age', 'campaign', 'previous', 'cons.conf.idx', 'euribor3m']\n",
        "\n",
        "# Filtramos las variables que seleccionamos\n",
        "X_t = X_train[variables_categoricas + variables_numericas]\n",
        "\n",
        "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
        "                            ])\n",
        "\n",
        "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
        "                                 ('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),      #podríamos no ponerlo, y que deje \"desconocido\" como una categoría más\n",
        "                                 ('cat', OneHotEncoder())])\n",
        "\n",
        "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
        "                                   ('cat', pipeline_categorico, variables_categoricas),\n",
        "                                  ])\n",
        "\n",
        "pipeline_modelo = Pipeline([('preprocess', pipeline_completo),\n",
        "                            ('xgb', xgb.XGBClassifier(seed=0))])"
      ],
      "id": "IJFPo3GZl3Pj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGzVBzs3mMST"
      },
      "source": [
        "#Solo Pre-procesamiento\n",
        "train = pipeline_completo.fit_transform(X_t)\n",
        "val = pipeline_completo.transform(X_v)"
      ],
      "id": "HGzVBzs3mMST",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7WBCwU5mSub",
        "outputId": "e012b9cb-4600-4b8f-d633-0a2554252ab6"
      },
      "source": [
        "xgb=xgb.XGBClassifier(seed=0)\n",
        "xgb.fit(train, y_train)"
      ],
      "id": "N7WBCwU5mSub",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eePmzFAzmkcS",
        "outputId": "4bef9cad-a44d-4e90-8e4a-9b284b1886b8"
      },
      "source": [
        "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
        "print(classification_report(y_train, xgb.predict(train)))\n",
        "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
        "print(classification_report(y_val, xgb.predict(val)))"
      ],
      "id": "eePmzFAzmkcS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MÉTRICAS CONJUNTO DE TRAIN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.99      0.95     23390\n",
            "           1       0.72      0.22      0.33      2970\n",
            "\n",
            "    accuracy                           0.90     26360\n",
            "   macro avg       0.81      0.60      0.64     26360\n",
            "weighted avg       0.89      0.90      0.88     26360\n",
            "\n",
            "MÉTRICAS CONJUNTO DE VALIDACIÓN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.99      0.95      5848\n",
            "           1       0.70      0.18      0.29       742\n",
            "\n",
            "    accuracy                           0.90      6590\n",
            "   macro avg       0.80      0.58      0.62      6590\n",
            "weighted avg       0.88      0.90      0.87      6590\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duAEjUzgmqxT"
      },
      "source": [
        "#Optimización de hiperparámetros\n",
        "#Grilla de parámetros\n",
        "params={'objective':[\"binary:logistic\",\"binary:hinge\"],\n",
        "        'learning_rate':[ 0.1,0.2,0.3],\n",
        "        'max_depth':[2,4, 6, 7, 8, 10],\n",
        "        'alpha':[2, 3, 5, 7],\n",
        "        \"n_estimators\":[5, 7, 10]\n",
        "       }\n",
        "xgb=xgb.XGBClassifier(seed=0)"
      ],
      "id": "duAEjUzgmqxT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atBORt9Gm-iX"
      },
      "source": [
        "#Búsqueda de parámetros\n",
        "cv_xgb = GridSearchCV(xgb, params, scoring='f1', cv=5,refit=True,n_jobs=-1)     \n",
        "cv_xgb.fit(train, y_train)"
      ],
      "id": "atBORt9Gm-iX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFXg8QhfnUtV"
      },
      "source": [
        "cv_xgb.best_params_\n",
        "#EN ESTA NOTEBOOK NO FUNCIONÓ, PERO EN LA OTRA ME DIO: "
      ],
      "id": "TFXg8QhfnUtV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIZ3RrwBysHk"
      },
      "source": [
        "GridSearchCV(cv=5, error_score=nan,\n",
        "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
        "                                     colsample_bylevel=1, colsample_bynode=1,\n",
        "                                     colsample_bytree=1, gamma=0,\n",
        "                                     learning_rate=0.1, max_delta_step=0,\n",
        "                                     max_depth=3, min_child_weight=1,\n",
        "                                     missing=None, n_estimators=100, n_jobs=1,\n",
        "                                     nthread=None, objective='binary:logistic',\n",
        "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
        "                                     scale_pos_weight=1, seed=0, silent=None,\n",
        "                                     subsample=1, verbosity=1),\n",
        "             iid='deprecated', n_jobs=-1,\n",
        "             param_grid={'alpha': [2, 3, 5, 7],\n",
        "                         'learning_rate': [0.1, 0.2, 0.3],\n",
        "                         'max_depth': [2, 4, 6, 7, 8, 10],\n",
        "                         'n_estimators': [5, 7, 10],\n",
        "                         'objective': ['binary:logistic', 'binary:hinge']},\n",
        "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
        "             scoring='f1', verbose=0)\n",
        "\n",
        "\n",
        "{'alpha': 2,\n",
        " 'learning_rate': 0.1,\n",
        " 'max_depth': 7,\n",
        " 'n_estimators': 7,\n",
        " 'objective': 'binary:hinge'}\n"
      ],
      "id": "TIZ3RrwBysHk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgL20Q1-y7hd"
      },
      "source": [
        "import xgboost as xgb"
      ],
      "id": "BgL20Q1-y7hd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mznSQCahnakS",
        "outputId": "51980563-b574-4ac4-81ba-8d5be8611385"
      },
      "source": [
        "#Entrenamiento de la mejor versión encontrada del modelo\n",
        "xgb_best = xgb.XGBClassifier(seed=0, alpha= 2, learning_rate= 0.1, max_depth= 7, n_estimators=7, objective='binary:hinge')\n",
        "xgb_best.fit(train, y_train)"
      ],
      "id": "mznSQCahnakS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(alpha=2, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
              "              min_child_weight=1, missing=None, n_estimators=7, n_jobs=1,\n",
              "              nthread=None, objective='binary:hinge', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmGT4nG5npUY",
        "outputId": "6e2c5456-608c-4394-d51d-87c5ae7715c1"
      },
      "source": [
        "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
        "print(classification_report(y_train, xgb_best.predict(train)))\n",
        "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
        "print(classification_report(y_val, xgb_best.predict(val)))"
      ],
      "id": "CmGT4nG5npUY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MÉTRICAS CONJUNTO DE TRAIN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.91      0.93     23390\n",
            "           1       0.46      0.57      0.50      2970\n",
            "\n",
            "    accuracy                           0.87     26360\n",
            "   macro avg       0.70      0.74      0.72     26360\n",
            "weighted avg       0.89      0.87      0.88     26360\n",
            "\n",
            "MÉTRICAS CONJUNTO DE VALIDACIÓN\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.91      0.93      5848\n",
            "           1       0.46      0.56      0.50       742\n",
            "\n",
            "    accuracy                           0.88      6590\n",
            "   macro avg       0.70      0.74      0.72      6590\n",
            "weighted avg       0.89      0.88      0.88      6590\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryZG090YnZfA"
      },
      "source": [
        "### Explainability"
      ],
      "id": "ryZG090YnZfA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFDNwJh0nZfA"
      },
      "source": [
        "Realizaremos feature importance y como opcional utilizaremos la librería SHAP para analizar las predicciones.\n"
      ],
      "id": "wFDNwJh0nZfA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpk3t1cvnZfB"
      },
      "source": [
        "### Presentación"
      ],
      "id": "lpk3t1cvnZfB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJZFMD1GnZfB"
      },
      "source": [
        "Al final del práctico, es necesario hacer 3 o 4 slides que irán incluidos en la presentación final.  \n",
        "Los slides deberán contener las etapas de preprocesamiento, los modelos que utilizamos, como optimizamos los hiperparámetros, cómo validamos y qué métricas utilizamos. Por último responderemos desde el punto de vista de negocio si sirve o no sirve el modelo."
      ],
      "id": "aJZFMD1GnZfB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIoO04oynZfB"
      },
      "source": [
        "### Librerías recomendadas"
      ],
      "id": "IIoO04oynZfB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-ir3gRPnZfC"
      },
      "source": [
        "Utilizaremos principalmente scikit-learn, opcionalmente xgboost y lightgbm.  \n",
        "Recomiendo el siguiente material:  \n",
        "\n",
        "- https://scikit-learn.org/stable/ -> Referencia de librería scikit-learn. Contiene casi todo lo que vamos a utilizar, pipelines, preprocesamiento y varios modelos.\n",
        "- https://xgboost.readthedocs.io/en/latest/ -> Librería muy utilizada debido a que tiene muy buenos resultados. Es un tipo de algoritmo \"boosting tree\"\n",
        "- https://lightgbm.readthedocs.io/en/latest/ -> Otra librería similar a xgboost, cada vez se usa más, debido a su facilidad de uso y buenos resultados.\n",
        "- https://shap.readthedocs.io/en/latest/index.html -> Librería SHAP, para realizar explainability y analizar predicciones.\n",
        "- https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py -> Ejemplo de pipelines, cross_validation y optimización de hiperparámetros"
      ],
      "id": "Z-ir3gRPnZfC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwjW60pZnZfC"
      },
      "source": [
        "### Feature importance y explainability"
      ],
      "id": "AwjW60pZnZfC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUb-LVGhnZfD"
      },
      "source": [
        "import xgboost as xgb\n",
        "variables_numericas = ['cons.price.idx', 'cons.conf.idx', 'age','duration']\n",
        "variables_categoricas = ['education', 'marital','job', 'contact', 'day_of_week']\n",
        "\n",
        "# Filtramos las variables que seleccionamos\n",
        "X_t = X_train[variables_categoricas + variables_numericas]\n",
        "\n",
        "pipeline_numerico = Pipeline([\n",
        "                             ('standard_scaler', StandardScaler()),\n",
        "                            ])\n",
        "\n",
        "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
        "                                   ('cat', OneHotEncoder(), variables_categoricas),\n",
        "                                  ])\n",
        "\n",
        "pipeline_modelo = Pipeline([('preprocess', pipeline_completo),\n",
        "                            ('xgb', xgb.XGBClassifier())])\n"
      ],
      "id": "XUb-LVGhnZfD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLypNsZHnZfD"
      },
      "source": [
        "pipeline_modelo.fit(X_t, y_train)"
      ],
      "id": "hLypNsZHnZfD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzcgK8-0nZfE"
      },
      "source": [
        "#### Obtener los nombres de las variables"
      ],
      "id": "YzcgK8-0nZfE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5MK9jY5nZfE"
      },
      "source": [
        "# Si realizamos one hot encoding, vamos a tener el problema de que se incrementan el numero de features y necesitamos la nueva lista.\n",
        "numeric_features = variables_numericas\n",
        "cat_features = pipeline_modelo.named_steps['preprocess'].transformers_[1][1].get_feature_names(variables_categoricas)"
      ],
      "id": "c5MK9jY5nZfE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQYAvDNunZfF"
      },
      "source": [
        "#### Feature importance utilizando XGBoost"
      ],
      "id": "bQYAvDNunZfF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtF1wdxznZfF"
      },
      "source": [
        "onehot_columns = np.array(cat_features)\n",
        "numeric_features_list = np.array(numeric_features)\n",
        "numeric_features_list = np.append(numeric_features_list, onehot_columns)"
      ],
      "id": "EtF1wdxznZfF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJGqpMNtnZfI"
      },
      "source": [
        "# Es necesario ordenar las los valores del feature importance (utilizamos argsort para tener el orden de los indices)\n",
        "sorted_idx = pipeline_modelo[1].feature_importances_.argsort()\n",
        "plt.barh(numeric_features_list[sorted_idx], pipeline_modelo[1].feature_importances_[sorted_idx])\n",
        "plt.xlabel(\"Xgboost Feature Importance\")\n",
        "plt.show()"
      ],
      "id": "IJGqpMNtnZfI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZTiyrKmnZfI"
      },
      "source": [
        "#### Feature importance utilizando eli5"
      ],
      "id": "nZTiyrKmnZfI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YCxQDH5nZfI"
      },
      "source": [
        "import eli5\n",
        "# Utilizar eli5 nos resuelve el problema de ordenar las columnas"
      ],
      "id": "9YCxQDH5nZfI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60XrJ_qinZfJ"
      },
      "source": [
        "onehot_columns = cat_features\n",
        "features_list = list(numeric_features)\n",
        "features_list.extend(onehot_columns)"
      ],
      "id": "60XrJ_qinZfJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGh5tXdKnZfJ"
      },
      "source": [
        "eli5.explain_weights(pipeline_modelo[1], top=50, feature_names=features_list)"
      ],
      "id": "WGh5tXdKnZfJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnUg7Kw8nZfJ"
      },
      "source": [
        "#### Utilizar SHAP para obtener feature importance y expainability de las predicciones (opcional)"
      ],
      "id": "qnUg7Kw8nZfJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfPe1GORnZfK"
      },
      "source": [
        "Como opcional, podemos utilizar la librería SHAP que nos muestra un tipo de explainability por cada predicción. Estas pueden ser agregadas para obtener un feature importance global del modelo."
      ],
      "id": "OfPe1GORnZfK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCBjD8rFnZfK"
      },
      "source": [
        "\n",
        "https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Census%20income%20classification%20with%20XGBoost.html"
      ],
      "id": "ZCBjD8rFnZfK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cQG8JVInZfK"
      },
      "source": [
        ""
      ],
      "id": "7cQG8JVInZfK",
      "execution_count": null,
      "outputs": []
    }
  ]
}